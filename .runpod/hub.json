{
  "title": "Runpod Worker Ollama",
  "description": "A serverless Ollama Worker for Runpod",
  "type": "serverless",
  "category": "language",
  "iconUrl": "https://ollama.com/public/ollama.png",
  "config": {
    "runsOn": "GPU",
    "gpuCount": 1,
    "gpuIds": "AMPERE_16,AMPERE_24,ADA_24",
    "containerDiskInGb": 20,
    "presets": [],
    "env": [
      {
        "key": "MODEL_NAMES",
        "input": {
          "name": "Model Names to preloadd",
          "type": "string",
          "description": "Name of models to preload, comma-separated",
          "default": "phi3",
          "advanced": false
        }
      },
      {
        "key": "MODEL_FILES",
        "input": {
          "name": "Model Files to create",
          "type": "string",
          "description": "Manual model files to create, comma-separated in the format name=path",
          "default": "",
          "advanced": true
        }
      },
      {
        "key": "MAX_CONCURRENCY",
        "input": {
          "name": "Max Concurrency",
          "type": "number",
          "description": "Maximum number of concurrent requests to handle (default: 8)",
          "default": 8,
          "advanced": true
        }
      },
      {
        "key": "OLLAMA_NUM_PARALLEL",
        "input": {
          "name": "Parallel Requests",
          "type": "string",
          "description": "Maximum number of concurrent requests to handle (default: 4 or 1)",
          "default": "",
          "advanced": true
        }
      }
    ]
  }
}